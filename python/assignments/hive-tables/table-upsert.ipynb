{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4268cdfb",
   "metadata": {},
   "source": [
    "In this notebook you will see how to upsert a Hive table with a new increment of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e827ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('table-upsert')\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/hive/accounts')\n",
    "\n",
    "checkpoint_dir = os.path.join(project_path, 'output/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop table if exists accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cca5f",
   "metadata": {},
   "source": [
    "### Create a new table\n",
    "\n",
    "* Take the data from the `users_base_path` and save it as a new table with the name `accounts`\n",
    "* Use [saveAsTable](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.saveAsTable.html#pyspark.sql.DataFrameWriter.saveAsTable)\n",
    "* as the location for the table use `accounts_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d43f6",
   "metadata": {},
   "source": [
    "### Verify that the table is created\n",
    "\n",
    "you can use the following SQL commands:\n",
    "* show tables\n",
    "* describe table_name\n",
    "* describe formatted table_name\n",
    "* describe extended table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cc09b-46e9-4f57-87d9-b96fec87d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808163a",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "\n",
    "* load the increment in to a Spark DataFrame \n",
    " * use the path `users_increment_path`\n",
    "* upsert the increment on the accounts table\n",
    " * use the approach with Union + row_number:\n",
    "   * add a new column `version` to both dataframes, use value 1 for the table and value 2 for the increment\n",
    "   * union both DataFrames using [unionByName](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionByName.html#pyspark.sql.DataFrame.unionByName)\n",
    "   * create a [window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.WindowSpec.partitionBy.html#pyspark.sql.WindowSpec.partitionBy) partitioned by user_id and sorted by the new `version` column\n",
    "   * call [row_number](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.row_number.html#pyspark.sql.functions.row_number) over this window\n",
    "   * this will allow you to use a filter to keep for each `user_id` only records with newer `version`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc782f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query for the upsert - create a new dataframe called `result`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69515ddb",
   "metadata": {},
   "source": [
    "#### Save the result\n",
    "\n",
    "Try to run the overwrite of the `accounts` table by this `result` DataFrame. \n",
    "\n",
    "Notice that running the overwrite will lead to the following error:\n",
    "\n",
    "`AnalysisException: Cannot overwrite table default.accounts that is also being read from`\n",
    "\n",
    "This is because we cannot write to the same location from which we also read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the overwrite to see the error:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a2cb7",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "This can be solved using checkpointing\n",
    "\n",
    "* Checkpoint the result DataFrame using [checkpoint](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html#pyspark.sql.DataFrame.checkpoint)\n",
    "* assign it to a new DataFrame\n",
    "* run the overwrite with this new checkpointed DataFrame\n",
    "\n",
    "Note:\n",
    "* the checkpoint will persist the data at a location specified using `setCheckpointDir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the checkpoint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the checkpointed result - the error should no longer be present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf834294",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
