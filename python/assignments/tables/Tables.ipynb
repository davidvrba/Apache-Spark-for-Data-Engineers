{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4268cdfb",
   "metadata": {},
   "source": [
    "# Working with tables\n",
    "\n",
    "In this notebook, we will see how to upsert a Hive table with new increment of data, we will try to make the operation more atomic and provide time-travel (roll-back) functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e827ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName('tables management')\n",
    "    .enableHiveSupport()\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/hive/accounts')\n",
    "\n",
    "accounts_output_path_v1 = os.path.join(project_path, 'output/tables/accounts/1')\n",
    "accounts_output_path_v2 = os.path.join(project_path, 'output/tables/accounts/2')\n",
    "\n",
    "checkpoint_dir = os.path.join(project_path, 'output/checkpoints')\n",
    "tmp_location = os.path.join(project_path, 'output/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop table if exists accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cca5f",
   "metadata": {},
   "source": [
    "### Create a new table\n",
    "\n",
    "* Take the data from the `users_base_path` and save it as a new table with the name `accounts`\n",
    "* Use [saveAsTable](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.saveAsTable.html#pyspark.sql.DataFrameWriter.saveAsTable)\n",
    "* as the location for the table use `accounts_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d43f6",
   "metadata": {},
   "source": [
    "### Verify that the table is created\n",
    "\n",
    "you can use the following SQL commands:\n",
    "* show tables\n",
    "* describe table_name\n",
    "* describe formatted table_name\n",
    "* describe extended table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808163a",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "\n",
    "* load the increment in to a Spark DataFrame \n",
    " * use the path `users_increment_path`\n",
    "* upsert the increment on the accounts table\n",
    " * use the approach with Union + row_number:\n",
    "   * add a new column `version` to both dataframes, use value 1 for the table and value 2 for the increment\n",
    "   * union both DataFrames using [unionByName](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unionByName.html#pyspark.sql.DataFrame.unionByName)\n",
    "   * create a [window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.WindowSpec.partitionBy.html#pyspark.sql.WindowSpec.partitionBy) partitioned by user_id and sorted by the new `version` column\n",
    "   * call [row_number](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.row_number.html#pyspark.sql.functions.row_number) over this window\n",
    "   * this will allow you to use a filter to keep for each `user_id` only records with newer `version`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc782f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the window:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query for the upsert - create a new dataframe called `result`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69515ddb",
   "metadata": {},
   "source": [
    "#### Save the result\n",
    "\n",
    "Try to run the overwrite of the `accounts` table by this `result` DataFrame. \n",
    "\n",
    "Notice that running the overwrite will lead to the following error:\n",
    "\n",
    "`AnalysisException: Cannot overwrite table default.accounts that is also being read from`\n",
    "\n",
    "This is because we cannot write to the same location from which we also read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the overwrite to see the error:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a2cb7",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "This can be solved using checkpointing\n",
    "\n",
    "* Checkpoint the result DataFrame using [checkpoint](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html#pyspark.sql.DataFrame.checkpoint)\n",
    "* assign it to a new DataFrame\n",
    "* run the overwrite with this new checkpointed DataFrame\n",
    "\n",
    "Note:\n",
    "* the checkpoint will persist the data at a location specified using `setCheckpointDir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the checkpoint:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the checkpointed result - the error should no longer be present:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaf9ac",
   "metadata": {},
   "source": [
    "### Atomicity\n",
    "\n",
    "If the saving process fails from some reason you may end up with a corrupted table. To avoid that, try to make the process more atomic. Do the saving again as follows:\n",
    "\n",
    "1. Crecreate the original table `accounts` at a new location (use `accounts_output_path_v1`)\n",
    "2. Do the upsert and save it at a different location, namaly accounts_output_path_v2, use a different name for the final table, namely `accounts_v2`\n",
    "3. Use SQL command `ALTER TABLE` to rename the `accounts` table to `accounts_delete`\n",
    "4. Use `ALTER TABLE` again to rename the `account_v2` to `accounts`\n",
    "5. Use SQL command `DROP TABLE` to delete `accounts_delete`\n",
    "\n",
    "Basicly, you will first write the result and after it is successfully written, you will switch the table names to make sure that your production table is still in a consistent state. On the other hand, if your write would fail from some reason, you woudn't make the switch to keep the original table in consistent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resave the original table at the location accounts_output_path_v1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the upsert - save the result at the location accounts_output_path_v2, use a new table_name (accounts_v2):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SQL commands to switch the names:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original table (the one that was renamed to accounts_delete):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1575b",
   "metadata": {},
   "source": [
    "### Time Travel\n",
    "\n",
    "Now imagine, that you have made a mistake and you actually don't want to do the upsert. We want to roll-back the operation. We can do it because the `DROP` command didn't delete the actual data, but only removed the information from the metastore. We can reconstruct the original data back so long we have the data and now the schema.\n",
    "\n",
    "1. Create an empty DataFrame with the schema of the accounts table (use the schema of the new table, because we didn't change it). To create an empty DataFrame use [createDataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame)\n",
    "2. Save the empty DataFrame at temporal location - use `tmp_location`\n",
    "3. Use ALTER TABLE command to change the location so the table points to the data before the upsert - `accounts_output_path_v1`\n",
    "4. Now the table is no longer empty so you can switch the names using ALTER TABLE to give it the proper name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02648e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as an empty table at temporal location:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the location of the empty table and switch the names\n",
    "# Drop the table with the wrong upsert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf834294",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark Delta Python 3.10.1",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
