{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ebbe7f-24c1-46b6-b395-f07b635468a2",
   "metadata": {},
   "source": [
    "# Schema evolution\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "* clone the table to a new location\n",
    "* append data which has one more column\n",
    "* append data which has different datatype in one column\n",
    "  * use [type widening](https://docs.delta.io/latest/delta-type-widening.html) since delta 4.0 (not available now)\n",
    "  * change the schema of the table\n",
    "  * append afterwords\n",
    "* drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d4330-9866-4012-9e1c-a8809281c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dc746-294b-4870-8041-84a8cf797098",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('delta-IV')\n",
    "    .config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.1')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a5756-c39d-4149-a29d-0c28ba354664",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/accounts')\n",
    "accounts_output_path_dev = os.path.join(project_path, 'output/accounts_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2f722-8b1f-4a3c-8c37-49201c4b7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First recreate the table using the data in the location accounts_output_path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72891e6e-9829-4182-9e4b-b2d6ed60d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the history of the table:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668a615-d872-48c2-8b8a-20af4edd5c09",
   "metadata": {},
   "source": [
    "## Create cloned table\n",
    "\n",
    "* Create a new table accounts_dev for testing purpose\n",
    "* Use [shallow clone](https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table)\n",
    "* Use the location `accounts_output_path_dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d0bbc-e667-4d95-b859-4915e74c7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528184b-6dc9-4300-b598-6cc526148f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the history of the cloned table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f486d1d-8697-44e1-9903-980e06f10a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or using SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d7660-019c-4a3d-9996-a86d6b116b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = [\n",
    "    (1, 'Test Test', 'This is testing account', 'Prague', 0, 1, 100, 1000, 'Test')\n",
    "]\n",
    "new_row_df = spark.createDataFrame(\n",
    "    new_row, \n",
    "    schema=['user_id', 'display_name', 'about', 'location', 'downvotes', 'upvotes', 'reputation', 'views', 'first_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b36d-8f85-42cd-85fb-992651b41a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf94577-b8da-4ffe-9587-8c721ff51e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the new_row_df and see how it differs from the schema of accounts_dev:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566b917-f528-46a2-b62d-42f5a012be75",
   "metadata": {},
   "source": [
    "## Append the new_row_df to the accounts_dev table\n",
    "\n",
    "* Use saveAsTable with the append mode\n",
    "  * it will fail with a schema mismatch error\n",
    "* Do one of the following:\n",
    "  *  set this config to True: `spark.databricks.delta.schema.autoMerge.enabled`\n",
    "  *  use `mergeSchema` option on the writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b7591-582e-436e-94af-fe1d04eeea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value of the conf setting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe3db7-183f-4078-a629-f80acd3cf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard saveAsTable fails with a schema mismatch detected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11312c98-fbcc-4ea7-af45-171e69723080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mergeSchema option:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed875c-4208-454b-9d3e-818283390b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the dev table to see if the column was added:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346f59e-85e8-4d7f-9c38-a368c4efae2b",
   "metadata": {},
   "source": [
    "## Append a new row where one of the columns has different data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86190d-7f0c-4bcd-95ad-d812134aad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we change the upvotes column data type to double:\n",
    "\n",
    "new_row = [\n",
    "    (2, 'Another Test', 'This is testing account', 'Prague', 0, 1.0, 100, 1000, 'Another')\n",
    "]\n",
    "new_row_df = spark.createDataFrame(\n",
    "    new_row, \n",
    "    schema=['user_id', 'display_name', 'about', 'location', 'downvotes', 'upvotes', 'reputation', 'views', 'first_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece9db8-ffd4-4799-9b0e-b4fd775804f6",
   "metadata": {},
   "source": [
    "* use saveAsTable with append mode\n",
    "  * it will fail with `Failed to merge fields` error\n",
    "* overwrite the table accounts_dev and change the data type in the table using [cast](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.cast.html#pyspark-sql-column-cast)\n",
    "  * use saveAsTable with the `overwriteSchema` option\n",
    "* then append the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19319cc2-2cdb-4805-98f2-80c5f8f759ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard saveAsTable fails with Failed to merge fields 'upvotes' and 'upvotes':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87add8-1243-4a06-8b1e-5dc9384a99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call saveAsTable on accounts_dev and use overwriteSchema option:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab817bb-19bb-4a83-a0eb-4365dc9c83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the schema of the table was changed, append the new data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710b332-17b3-4d4a-8d9f-81aca8994116",
   "metadata": {},
   "source": [
    "## Drop a column from the schema\n",
    "\n",
    "* drop the `about` column from the accounts_dev table\n",
    "* use alter table drop column\n",
    "  * you will need to set this conf setting: delta.columnMapping.mode = \"name\"\n",
    "  * first check the tblproperties to see the minReaderVersion and minWriterVersion - you will need the values (2,5). If yo don't have them, you need to set it first\n",
    "  * see [docs](https://docs.delta.io/latest/delta-column-mapping.html#how-to-enable-delta-lake-column-mapping) for columnMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a016af3-3ea1-41f1-a4f5-0fe5ac6934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tblproperties:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296ca1d-c3c5-4b2d-931b-05047be6de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the properties:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f60b0d-3f74-4475-afb2-f8fb6a35171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770d7bb-b6c9-4b64-899f-d87a21d4400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the schema after the change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d3e97-5af5-4a94-88e3-4b49de400d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
