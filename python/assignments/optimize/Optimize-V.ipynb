{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the query plan II\n",
    "\n",
    "Suppose we want to join questions with users (using the table from the metastore). We also want to use a UDF (which does some computation on the question message field) and using a window we want for each user to order the questions he/she asked depending on the creation date. \n",
    "\n",
    "See the query bellow which does that in suboptimal way and try to rewrite it to achieve more optimal plan. More specifically try to eliminate the Exchange operator in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, row_number\n",
    ")\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Optimize II')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'data/questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will turn broadcast join off because we want to work with sort merge join (SMJ) because we want to assume that\n",
    "# in practice both datasets are large so SMJ would manifest anyway\n",
    "\n",
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.table('users')\n",
    "\n",
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', questions_input_path)\n",
    "    .load()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF:\n",
    "\n",
    "The UDF bellow is just simple function that gets the lenght of a string. This can be easily done using native pyspark dataframe function length. For the sake of this example however suppose that this function encapsulates some complex logic which can not be done natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(IntegerType())\n",
    "def get_length_udf(str):\n",
    "    return len(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().partitionBy('user_id').orderBy('creation_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "\n",
    "The query bellow is suboptimal. Try to rewrite the query to achive more optimal plan that leads to more efficient execution.\n",
    "\n",
    "Hint:\n",
    "* see the query plan\n",
    "* eliminate the Exchange from the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .join(questionsDF, 'user_id')\n",
    "    .withColumn('question_len', get_length_udf('body'))\n",
    "    .withColumn('question_n', row_number().over(w))\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite the query:\n",
    "\n",
    "Hint:\n",
    "* move the UDF before the join - this will preserve your partitioning and will eliminate the suffle for the window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark Delta Python 3.10.1",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
