{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36af78e7",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "In this notebook you will answer a simple analytical question but as you will see it might be tricky to obtain a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Debugging-I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "usersP_output_path =  os.path.join(project_path, 'data/usersP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435ef46",
   "metadata": {},
   "source": [
    "1. Read the data that represent users from the path `usersP_output_path`\n",
    " * the dataset is partitioned by `last2_id` which are last two digits of the `user_id`. So for example a user with user_id = 540 would be in the partition `40`\n",
    "2. Find out how many users are together in the partitions `02` and `03`\n",
    " * try more different ways how to retrieve the result. \n",
    "3. Can you see the problem? If yes, can you explain what happened and determine what should be the correct approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read.parquet(usersP_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c006f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the users using the isin function:\n",
    "\n",
    "(\n",
    "  users\n",
    "  .filter(col('last2_id').isin(['02', '03']))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the users using `==` and `|` operators\n",
    "\n",
    "(\n",
    "  users\n",
    "  .filter((col('last2_id') == '02') | (col('last2_id') == '03'))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a51b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data directly from the path /last2_id=02 and from the path /last2_id=03\n",
    "\n",
    "spark.read.parquet(\n",
    "    usersP_output_path + '/last2_id=02'\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\n",
    "    usersP_output_path + '/last2_id=03'\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7abcb",
   "metadata": {},
   "source": [
    "* The problem is that Spark infers the last2_id as a long type and it will cast the values 02 and 03 to 2 and 3, so it will not find any rows when using isin function.\n",
    "\n",
    "* with the == operator it will cast the right side to long and both partitions 2 and 02 will be taken into account (because there is also a user with id user_id=2 which has partition `2`)\n",
    "\n",
    "* The solution to the problem is either to read the data with schema where we tell spark that the datatype is string, or we disable the schema inference of the partition column `spark.sql.sources.partitionColumnTypeInference.enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the schema of users\n",
    "\n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new schema where the last2_id is string\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField('user_id', LongType()),\n",
    "    StructField('display_name', StringType()),\n",
    "    StructField('about', StringType()),\n",
    "    StructField('location', StringType()),\n",
    "    StructField('downvotes', LongType()),\n",
    "    StructField('upvotes', LongType()),\n",
    "    StructField('reputation', LongType()),\n",
    "    StructField('views', LongType()),\n",
    "    StructField('last2_id', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data again with the schema\n",
    "\n",
    "users = spark.read.schema(users_schema).parquet(usersP_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the isin function\n",
    "\n",
    "(\n",
    "  users\n",
    "  .filter(col('last2_id').isin(['02', '03']))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aadb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try it also with the conf setting that will dispable the type inference for partitioning\n",
    "\n",
    "spark.conf.set('spark.sql.sources.partitionColumnTypeInference.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b662dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you disable the setting, try the filter with isin and with == again\n",
    "users = spark.read.parquet(usersP_output_path)\n",
    "\n",
    "(\n",
    "  users\n",
    "  .filter(col('last2_id').isin(['02', '03']))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  users\n",
    "  .filter((col('last2_id') == '02') | (col('last2_id') == '03'))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb892548",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark Delta",
   "language": "python",
   "name": "pyspark-delta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
