{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d34b3-ab9c-420b-a437-d4c6c5cc49dc",
   "metadata": {},
   "source": [
    "In this notebook you will\n",
    "\n",
    "* think about atomic upsert of Hive tables\n",
    "* implement time-travel feature\n",
    "* do a simple schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149f258-8eb1-4658-a786-260884ae388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc8c6d-4ef7-4139-bd1f-bc93c7cb59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('time-travel')\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a3190-7b25-4ace-a86d-b24ea27ab624",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "\n",
    "accounts_output_path_v1 = os.path.join(project_path, 'output/tables/accounts/1')\n",
    "accounts_output_path_v2 = os.path.join(project_path, 'output/tables/accounts/2')\n",
    "\n",
    "tmp_location = os.path.join(project_path, 'output/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b22bb9-cbd0-412b-a5aa-31c00b73434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop table if exists accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a7184-9e0c-42ef-8b9b-859a3096310d",
   "metadata": {},
   "source": [
    "### Atomicity\n",
    "\n",
    "If the saving process fails from some reason you may end up with a corrupted table. To avoid that, try to make the process more atomic. Do the saving again as follows:\n",
    "\n",
    "1. Create the original table `accounts` at a new location (use `accounts_output_path_v1`)\n",
    "2. Do the upsert and save it at a different location, namely accounts_output_path_v2, use a different name for the final table, namely `accounts_v2`\n",
    "3. Use SQL command `ALTER TABLE` to rename the `accounts` table to `accounts_delete`\n",
    "4. Use `ALTER TABLE` again to rename the `account_v2` to `accounts`\n",
    "5. Use SQL command `DROP TABLE` to delete `accounts_delete`\n",
    "6. Check how many locations are not null before and after the upsert\n",
    "\n",
    "Basically, you will first write the result and after it is successfully written, you will switch the table names to make sure that your production table is still in a consistent state. On the other hand, if your write would fail for some reason, you won't make the switch to keep the original table in a consistent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f4bdf-b539-4c4f-a04d-77ccf821b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resave the original table at the location accounts_output_path_v1\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', accounts_output_path_v1)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904f402-cb0a-4f6b-b109-7c3f98369d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n",
    "\n",
    "increment = spark.read.parquet(users_increment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06235db6-69ee-494e-b573-63744471b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many locations are not null\n",
    "\n",
    "spark.table('accounts').filter(col('location').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb8361-e823-4fd9-b697-845038fff993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the window\n",
    "\n",
    "w = Window().partitionBy('user_id').orderBy(desc('version'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20aa81-f780-4ae9-94f7-b19036001192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the upsert - save the result at the location accounts_output_path_v2, use a new table_name (accounts_v2)\n",
    "\n",
    "result = (\n",
    "    spark.table('accounts').withColumn('version', lit(1))\n",
    "    .unionByName(\n",
    "        increment.withColumn('version', lit(2))\n",
    "    )\n",
    "    .withColumn('r', row_number().over(w))\n",
    "    .filter(col('r') == 1)\n",
    "    .drop('r', 'version')\n",
    ")\n",
    "\n",
    "(\n",
    "    result\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', accounts_output_path_v2)\n",
    "    .saveAsTable('accounts_v2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5e673-20f4-4985-bde9-09a4ec4d3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SQL commands to switch the names\n",
    "spark.sql('drop table if exists accounts_delete')\n",
    "\n",
    "spark.sql('ALTER TABLE accounts RENAME TO accounts_delete')\n",
    "spark.sql('ALTER TABLE accounts_v2 RENAME TO accounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44588f9-d6c3-430e-b97a-19447bf33c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original table (the one that was renamed to accounts_delete)\n",
    "\n",
    "spark.sql('DROP TABLE accounts_delete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f2601-a62d-4307-8cf5-bd0d1bd497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again how many locations are not null:\n",
    "\n",
    "spark.table('accounts').filter(col('location').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d4f63-59ae-40d7-ba1c-e0a9df208c8a",
   "metadata": {},
   "source": [
    "### Time Travel\n",
    "\n",
    "Now imagine, that you have made a mistake and you actually don't want to do the upsert. We want to roll-back the operation. We can do it because the `DROP` command didn't delete the actual data, but only removed the information from the metastore. We can reconstruct the original data back so long we have the data and now the schema.\n",
    "\n",
    "1. Create an empty DataFrame with the schema of the accounts table (use the schema of the new table, because we didn't change it). To create an empty DataFrame use [createDataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame)\n",
    "2. Save the empty DataFrame at temporal location - use `tmp_location`\n",
    "3. Use ALTER TABLE command to change the location so the table points to the data before the upsert - `accounts_output_path_v1`\n",
    "4. Now the table is no longer empty so you can switch the names using ALTER TABLE to give it the proper name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4e4b1-8c90-4bbd-8b00-08f1d67abad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty DataFrame:\n",
    "\n",
    "empty_table = spark.createDataFrame([], spark.table('accounts').schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db707e4b-b5d7-4fdc-9b61-bd7601e74b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as an empty table at temporal location\n",
    "(\n",
    "    empty_table\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', tmp_location)\n",
    "    .saveAsTable('accounts_empty')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bb9f9-0647-4939-acae-5bc451e84453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the location of the empty table and switch the names\n",
    "# Drop the table with the wrong upsert\n",
    "\n",
    "spark.sql('ALTER TABLE accounts RENAME TO accounts_to_delete')\n",
    "\n",
    "spark.sql(f'ALTER TABLE accounts_empty SET LOCATION \"{accounts_output_path_v1}\"')\n",
    "\n",
    "spark.sql('ALTER TABLE accounts_empty RENAME TO accounts')\n",
    "\n",
    "spark.sql('DROP TABLE accounts_to_delete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675195e2-793f-4176-83f2-87a6641f0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again how many locations are not null:\n",
    "\n",
    "spark.table('accounts').filter(col('location').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb759575-9427-4353-86d7-a201e24a41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('accounts').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b082b-bced-4201-baed-ecf8631155a6",
   "metadata": {},
   "source": [
    "## Schema evolution - drop the column `about`\n",
    "\n",
    "* First try to use alter table drop column statement\n",
    "  * this won't work\n",
    "* use similar approach as before\n",
    "  * create a new table at empty location and for this new table use modified schema without the column\n",
    "  * then change the location of the empty table to point to the data\n",
    "  * finally switch the names\n",
    "  * verify that the modified table has a new schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c6dbe-5f75-4548-ac41-486c207cf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails with: The feature is not supported\n",
    "\n",
    "#spark.sql('alter table accounts drop column about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f507721-2811-4083-8a84-9402a7188717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty DataFrame without the about column:\n",
    "\n",
    "empty_table = spark.createDataFrame([], spark.table('accounts').drop('about').schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05b233-1802-4d5b-aa98-e55acece8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as an empty table at temporal location\n",
    "(\n",
    "    empty_table\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', tmp_location)\n",
    "    .saveAsTable('accounts_empty')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8093234-9af2-4db1-a792-17bbe9458c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the location of the empty table and switch the names\n",
    "# Drop the original table\n",
    "\n",
    "spark.sql('ALTER TABLE accounts RENAME TO accounts_to_delete')\n",
    "\n",
    "spark.sql(f'ALTER TABLE accounts_empty SET LOCATION \"{accounts_output_path_v1}\"')\n",
    "\n",
    "spark.sql('ALTER TABLE accounts_empty RENAME TO accounts')\n",
    "\n",
    "spark.sql('DROP TABLE accounts_to_delete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c11ba5-8dec-4707-9e89-1c818e9f28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the schema:\n",
    "\n",
    "spark.table('accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c16213-f6c4-46c9-80fd-bf4c6050f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('accounts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7a235-7527-43f4-9b08-21ac79264c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
