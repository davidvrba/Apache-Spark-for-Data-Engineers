{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155e5af1-8a87-492f-b188-171052555799",
   "metadata": {},
   "source": [
    "# Advanced Features of Delta-Lake tables\n",
    "\n",
    "In this notebook you will\n",
    "* test the Liquid Clustering (LC)\n",
    "* test Deletion Vectors for merging increment to the table\n",
    "* merge the increment into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db1dea-c438-4d3b-a6ab-e303b5ae6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afa1d8-d166-40b8-ba2f-bde6ccbb5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('advanced-features')\n",
    "    .config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.1')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4d76e-d0f3-4865-985f-83901a697d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "questions = os.path.join(project_path, 'data/questions')\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/accounts')\n",
    "accounts_output_path_v2 = os.path.join(project_path, 'output/accounts_v2')\n",
    "testing_output = os.path.join(project_path, 'output/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317b112-b938-4a3e-a345-175b17823b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop table if exists accounts')\n",
    "spark.sql('drop table if exists accounts_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53527d23-85fe-4396-9e03-b20bb2462786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the table accounts:\n",
    "\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('delta')\n",
    "    .option('path', accounts_output_path)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc38c99-9f17-4904-8931-2026d7b95aca",
   "metadata": {},
   "source": [
    "## Turn on liquid clustering\n",
    "\n",
    "* Create a table accounts_v2 on location accounts_output_path_v2\n",
    "* Turn on liquid clustering on the table\n",
    "* Fill the table with data from the accounts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a80d8-b740-45eb-90d7-ff132ac426fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table accounts_v2 (it is not possible to turn on LC on existing table)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE accounts_v2 USING DELTA \n",
    "    CLUSTER BY (user_id)\n",
    "    LOCATION '{accounts_output_path_v2}'\n",
    "    AS SELECT * from accounts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1a191-300a-4072-a304-14972c8b19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the table properties of the accounts table:\n",
    "\n",
    "spark.sql('show tblproperties accounts').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f8913-ec14-4e21-8c97-dc760d8beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the table properties of the accounts_v2 table:\n",
    "# See the minReaderVersion/minWriterVersion\n",
    "\n",
    "spark.sql('show tblproperties accounts_v2').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828459e-f6d9-46aa-8a6a-57e2a61e6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see it using sql: desc detail\n",
    "\n",
    "spark.sql('desc detail accounts').select('properties', 'minReaderVersion', 'minWriterVersion').show(n=100, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af7f89-1c2b-4bdb-90ef-8e79dbb5a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('desc detail accounts_v2').select('properties', 'minReaderVersion', 'minWriterVersion').show(n=100, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dfa67-3759-4393-a1bd-d6e93a1e50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8107ad-f2fa-4942-a657-420573210800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the accounts table\n",
    "# measure the execution time using the time module\n",
    "# use the noop operator for the write\n",
    "# do the same for the accounts_v2 table\n",
    "\n",
    "start_time = time.time()\n",
    "(\n",
    "    spark.table('accounts')\n",
    "    .filter(col('user_id') > 10000)\n",
    "    .distinct()\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution takes: {execution_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb68d90-8284-49bc-b37c-182574030b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for accounts_v2:\n",
    "\n",
    "start_time = time.time()\n",
    "(\n",
    "    spark.table('accounts_v2')\n",
    "    .filter(col('user_id') > 10000)\n",
    "    .distinct()\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution takes: {execution_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4b6b1-88e0-46c9-bcf3-4b992c4e255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run optimize command on both tables:\n",
    "\n",
    "spark.sql('optimize accounts_v2')\n",
    "spark.sql('optimize accounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9eff69-2d95-4870-9e34-7326a7e148fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call vacuum on the tables:\n",
    "\n",
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)\n",
    "\n",
    "spark.sql('vacuum accounts RETAIN 0 HOURS ')\n",
    "\n",
    "spark.sql('vacuum accounts_v2 RETAIN 0 HOURS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e976f-96da-44e6-8dc4-2070be438702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now measure the execution time again:\n",
    "\n",
    "start_time = time.time()\n",
    "(\n",
    "    spark.table('accounts')\n",
    "    .filter(col('user_id') > 10000)\n",
    "    .distinct()\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution takes: {execution_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c11198-4a50-4c70-838d-c6a2cfe7ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "(\n",
    "    spark.table('accounts_v2')\n",
    "    .filter(col('user_id') > 10000)\n",
    "    .distinct()\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution takes: {execution_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72ce36-0c1d-4132-bac5-81012b3565fb",
   "metadata": {},
   "source": [
    "## Deletion Vectors\n",
    "\n",
    "* turn on the feature on account_v2\n",
    "* check the table properties\n",
    "* merge the increment to the table\n",
    "* check the folder with the data and see the bin file\n",
    "* run optimize & vacuum and check the folder again\n",
    "* the noop operator doesn't work on the table with the deletion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a855410-7967-4f54-89da-36ace93266d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('ALTER TABLE accounts_v2 SET TBLPROPERTIES (delta.enableDeletionVectors = true)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a84ee-a719-41b2-b46d-e52933b0eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tblproperties accounts_v2').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572d556-59ed-4f31-9ad0-d1efae656697",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment = spark.read.parquet(users_increment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de2546-e5ac-40fa-9f21-d2397b0ee3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    DeltaTable.forName(spark, 'accounts_v2')\n",
    "    .alias('accounts')\n",
    "    .merge(\n",
    "        increment.alias('increment'),\n",
    "        'accounts.user_id == increment.user_id'\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6a1b5-da85-4950-8577-3be2d6612094",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('optimize accounts_v2')\n",
    "\n",
    "spark.sql('vacuum accounts_v2 RETAIN 0 HOURS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03407de4-1315-4c36-9982-759e4898c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails with: requirement failed: Cannot work with a non-pinned table snapshot of the TahoeFileIndex\n",
    "(\n",
    "    spark.table('accounts_v2')\n",
    "    .filter(col('user_id') > 10000)\n",
    "    .distinct()\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    # .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f00a6d-70f9-46fc-94eb-106325c64d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
