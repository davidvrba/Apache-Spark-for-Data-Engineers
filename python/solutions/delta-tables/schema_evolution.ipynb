{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ebbe7f-24c1-46b6-b395-f07b635468a2",
   "metadata": {},
   "source": [
    "# Schema evolution\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "* clone the table to a new location\n",
    "* append data which has one more column\n",
    "* drop a column\n",
    "* append data which has different datatype in one column\n",
    "  * use [type widening](https://docs.delta.io/latest/delta-type-widening.html) since delta 4.0 (not available now)\n",
    "  * change the schema of the table\n",
    "  * append afterwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d4330-9866-4012-9e1c-a8809281c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dc746-294b-4870-8041-84a8cf797098",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('delta-IV')\n",
    "    .config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.1')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a5756-c39d-4149-a29d-0c28ba354664",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/accounts')\n",
    "accounts_output_path_dev = os.path.join(project_path, 'output/accounts_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2f722-8b1f-4a3c-8c37-49201c4b7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First recreate the table using the data in the location accounts_output_path:\n",
    "\n",
    "spark.sql('drop table if exists accounts')\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE accounts\n",
    "    USING DELTA\n",
    "    LOCATION '{accounts_output_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72891e6e-9829-4182-9e4b-b2d6ed60d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the history of the table:\n",
    "\n",
    "DeltaTable.forPath(spark, accounts_output_path).history().select('version', 'timestamp', 'operation').show(truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668a615-d872-48c2-8b8a-20af4edd5c09",
   "metadata": {},
   "source": [
    "## Create cloned table\n",
    "\n",
    "* Create a new table accounts_dev for testing purpose\n",
    "* Use [shallow clone](https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table)\n",
    "* Use the location `accounts_output_path_dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d0bbc-e667-4d95-b859-4915e74c7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "spark.sql('drop table if exists accounts_dev')\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE accounts_dev\n",
    "    SHALLOW CLONE accounts\n",
    "    LOCATION '{accounts_output_path_dev}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528184b-6dc9-4300-b598-6cc526148f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the history of the cloned table\n",
    "\n",
    "DeltaTable.forPath(spark, accounts_output_path_dev).history().select('version', 'timestamp', 'operation').show(truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f486d1d-8697-44e1-9903-980e06f10a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ore using SQL:\n",
    "\n",
    "spark.sql('describe history accounts_dev').select('version', 'timestamp', 'operation').show(truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d7660-019c-4a3d-9996-a86d6b116b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = [\n",
    "    (1, 'Test Test', 'This is testing account', 'Prague', 0, 1, 100, 1000, 'Test')\n",
    "]\n",
    "new_row_df = spark.createDataFrame(\n",
    "    new_row, \n",
    "    schema=['user_id', 'display_name', 'about', 'location', 'downvotes', 'upvotes', 'reputation', 'views', 'first_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b36d-8f85-42cd-85fb-992651b41a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf94577-b8da-4ffe-9587-8c721ff51e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the new_row_df and see it differs from the schema of accounts_dev:\n",
    "\n",
    "new_row_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61746b-cb0a-4240-be1d-2996a78918b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('accounts_dev').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566b917-f528-46a2-b62d-42f5a012be75",
   "metadata": {},
   "source": [
    "## Append the new_row_df to the accounts_dev table\n",
    "\n",
    "* Use saveAsTable with the append mode\n",
    "  * it will fail with a schema mismatch error\n",
    "* Do one of the following:\n",
    "  *  set this config to True: `spark.databricks.delta.schema.autoMerge.enabled`\n",
    "  *  use `mergeSchema` option on the writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b7591-582e-436e-94af-fe1d04eeea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value of the conf setting:\n",
    "\n",
    "spark.conf.get('spark.databricks.delta.schema.autoMerge.enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe3db7-183f-4078-a629-f80acd3cf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails with A schema mismatch detected\n",
    "\n",
    "(\n",
    "    new_row_df\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .option('path', accounts_output_path)\n",
    "    #.saveAsTable('accounts_dev')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11312c98-fbcc-4ea7-af45-171e69723080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mergeSchema option:\n",
    "\n",
    "(\n",
    "    new_row_df\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .option('path', accounts_output_path)\n",
    "    .option('mergeSchema', True)\n",
    "    .saveAsTable('accounts_dev')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed875c-4208-454b-9d3e-818283390b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the dev table to see if the column was added:\n",
    "\n",
    "spark.table('accounts_dev').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346f59e-85e8-4d7f-9c38-a368c4efae2b",
   "metadata": {},
   "source": [
    "## Append a new row where one of the columns has different data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86190d-7f0c-4bcd-95ad-d812134aad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we change the upvotes column data type to double:\n",
    "\n",
    "new_row = [\n",
    "    (2, 'Another Test', 'This is testing account', 'Prague', 0, 1.0, 100, 1000, 'Another')\n",
    "]\n",
    "new_row_df = spark.createDataFrame(\n",
    "    new_row, \n",
    "    schema=['user_id', 'display_name', 'about', 'location', 'downvotes', 'upvotes', 'reputation', 'views', 'first_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123130f-2ed1-48e6-a9c9-09662b1c8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece9db8-ffd4-4799-9b0e-b4fd775804f6",
   "metadata": {},
   "source": [
    "* use saveAsTable with append mode\n",
    "  * it will fail with `Failed to merge fields` error\n",
    "* overwrite the table accounts_dev and change the data type in the table using [cast](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.cast.html#pyspark-sql-column-cast)\n",
    "  * use saveAsTable with the `overwriteSchema` option\n",
    "* then append the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19319cc2-2cdb-4805-98f2-80c5f8f759ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails with Failed to merge fields 'upvotes' and 'upvotes'\n",
    "\n",
    "(\n",
    "    new_row_df\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .option('path', accounts_output_path)\n",
    "    .option('mergeSchema', True)\n",
    "    #.saveAsTable('accounts_dev')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87add8-1243-4a06-8b1e-5dc9384a99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call saveAsTable on accounts_dev and use overwriteSchema option:\n",
    "\n",
    "(\n",
    "    spark.table('accounts_dev')\n",
    "    .withColumn('upvotes', col('upvotes').cast('double'))\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('delta')\n",
    "    .option('overwriteSchema', True)\n",
    "    .saveAsTable('accounts_dev')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab817bb-19bb-4a83-a0eb-4365dc9c83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the schema of the table was changed, append the new data:\n",
    "\n",
    "(\n",
    "    new_row_df\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .option('path', accounts_output_path)\n",
    "    .saveAsTable('accounts_dev')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ca7e4-5954-4bfa-a06b-c8f3d2d514af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data type back:\n",
    "\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('overwrite')\n",
    "    .option('path', accounts_output_path)\n",
    "    .option('overwriteSchema', True)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f60b0d-3f74-4475-afb2-f8fb6a35171e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
