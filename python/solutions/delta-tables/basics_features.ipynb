{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2382601e",
   "metadata": {},
   "source": [
    "# Delta Tables\n",
    "\n",
    "* In this notebook you will learn basic functionality of Delta-lake\n",
    "\n",
    "## Useful links:\n",
    "* [User Guide](https://docs.delta.io/latest/delta-batch.html)\n",
    "* [Python API](https://docs.delta.io/latest/api/python/spark/index.html)\n",
    "* [Delta Protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md#delta-transaction-log-protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d004aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b3df6-d405-4e4d-a4b5-07417bec000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('delta-conf')\n",
    "    .config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.1')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ac4ad-222c-4e10-9014-b274f955f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef829dd8",
   "metadata": {},
   "source": [
    "### Create a Delta table\n",
    "\n",
    "* First drop the table accounts if it already exists\n",
    "  * use sql command to drop the table\n",
    "  * see [docs](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-drop-table.html#drop-table) for the drop command\n",
    "  * be aware that this doesn't remove the actual files, it just removes the information from metastore. You need to go and delete the actual files manually if they exist\n",
    "* Take the data from the `users_base_path` and save it as a Delta table with the name `accounts`\n",
    "* as the location for the table use `accounts_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the table if it exists:\n",
    "\n",
    "spark.sql('drop table if exists accounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a new table accounts from the data\n",
    "# use the format delta when saving\n",
    "\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('delta')\n",
    "    .option('path', accounts_output_path)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362d4b9",
   "metadata": {},
   "source": [
    "### Verify that the table is created\n",
    "\n",
    "you can use the following SQL commands:\n",
    "* [show tables](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-show-tables.html)\n",
    "* [describe extended](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-describe-table.html) table_name\n",
    "* [describe detail](https://docs.delta.io/latest/delta-utility.html#retrieve-delta-table-details) table_name\n",
    "\n",
    "you can also use the API of the Delta table:\n",
    "* create the delta table object using the [DeltaTable.forName](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaTable.forName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25066a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the table was successfully created:\n",
    "\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('desc extended accounts').show(truncate=False, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ea8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe detail accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa90c5a-fc88-43e2-991f-da2642309160",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe detail accounts').select('location', 'numFiles', 'sizeInBytes', 'properties').show(truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b73f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail() function called on the Delta table:\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').detail().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c1e5b",
   "metadata": {},
   "source": [
    "### Version history of the delta table\n",
    "\n",
    "See the history of the delta table. You can use:\n",
    "* SQL command: [describe history](https://docs.delta.io/latest/delta-utility.html#retrieve-delta-table-history) table_name\n",
    "* [history](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaTable.history) function on the Delta table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbecdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe history accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').history().select('version', 'timestamp', 'operation').show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7273c",
   "metadata": {},
   "source": [
    "### Upsert / Merge\n",
    "\n",
    "* load the increment into a Spark DataFrame \n",
    "  * use the path users_increment_path\n",
    "* upsert the increment on the accounts table (use merge)\n",
    "* Useful links for merge:\n",
    "  * docs for [merge](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge)\n",
    "  * [merge](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaMergeBuilder) api\n",
    "  * delta blog for [merge](https://delta.io/blog/2023-02-14-delta-lake-merge/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n",
    "\n",
    "increment = spark.read.parquet(users_increment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the merge:\n",
    "\n",
    "(\n",
    "    DeltaTable.forName(spark, 'accounts')\n",
    "    .alias('accounts')\n",
    "    .merge(\n",
    "        increment.alias('increment'),\n",
    "        'accounts.user_id == increment.user_id'\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the history again to see that we have created a new version\n",
    "\n",
    "spark.sql('describe history accounts').select('version', 'timestamp', 'operation', 'operationMetrics').show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff9817-b02e-48a3-8a09-81ede91ca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the field operaionMetrics see the keys: numTargetRowsInserted, numTargetRowsUpdated\n",
    "(\n",
    "    spark.sql('describe history accounts')\n",
    "    .select(\n",
    "        'version',\n",
    "        col('operationMetrics')['numTargetRowsInserted'],\n",
    "        col('operationMetrics')['numTargetRowsUpdated']\n",
    "    )\n",
    ").show(truncate=130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dcc01",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "\n",
    "After you are done with the writes on the delta table, it might be useful to call optimize on it.\n",
    "\n",
    "* call [optimize](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaOptimizeBuilder)\n",
    "* use [z-order](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaOptimizeBuilder.executeZOrderBy) by the column `user_id`\n",
    "* other useful links:\n",
    "  *   https://docs.delta.io/latest/optimizations-oss.html#optimize-performance-with-file-management\n",
    "* check manually the files under the table to see that it was compacted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec03d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').optimize().executeZOrderBy('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81774717-6359-46fc-b51b-4156c6994699",
   "metadata": {},
   "source": [
    "### Vacuum\n",
    "\n",
    "* remove old files that you don't need anymore\n",
    "* run [vacuum](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaTable.vacuum) on the accounts delta table\n",
    "  * you can also use [SQL](https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table)\n",
    "* check that the files from the directory were removed\n",
    "\n",
    "Note:\n",
    "* if you want to remove files that are not older than 7 days, you will have to disable the following setting: `spark.databricks.delta.retentionDurationCheck.enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362561e3-6308-4dde-abf9-092736111530",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').vacuum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534a308",
   "metadata": {},
   "source": [
    "### Delete from delta\n",
    "\n",
    "* [delete](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaTable.delete) from the accounts table a row where user_id = 79\n",
    "* check that the row was really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b75641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete:\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').delete(col('user_id') == 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it was deleted:\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650d2f5",
   "metadata": {},
   "source": [
    "### Time travel\n",
    "\n",
    "* Now imagine, that you have made a mistake and you actually don't want to remove the user. Delta allows you to revert this operation. \n",
    "\n",
    "1) You can first take a look at a particular snapshot using the option `versionAsOf` on the DataFrameReader\n",
    "2) Then if you decide that you really want to revert your operation, you can use `restoreToVersion` on the Delta Table\n",
    "3) First check the table history to see in which version it was deleted\n",
    "\n",
    "useful links:\n",
    "* docs for [time travel](https://docs.delta.io/latest/delta-batch.html#query-an-older-snapshot-of-a-table-time-travel) feature\n",
    "* docs for [restoreToVersion](https://docs.delta.io/latest/api/python/spark/index.html#delta.tables.DeltaTable.restoreToVersion)\n",
    "* delta blog for [time travel](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)\n",
    "* delta blog for [rollback](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87fdfd-0091-48a2-8462-26fecbe1da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').history().select('version', 'timestamp', 'operation').show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c59af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the version of the table where the user wasn't deleted:\n",
    "\n",
    "spark.read.option('versionAsOf', 4).table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollback to the version\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').restoreToVersion(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ab248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the table to see that the row is present there\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how it was written to the history\n",
    "\n",
    "spark.sql('describe history accounts').select('version', 'timestamp', 'operation', 'operationMetrics').show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebc227-ff19-4e46-a3c3-4c4a85a7e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
