{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2382601e",
   "metadata": {},
   "source": [
    "# Delta Tables\n",
    "\n",
    "* In this notebook you will learn basic functionality of delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d004aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b3df6-d405-4e4d-a4b5-07417bec000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('delta-conf')\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.1\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ac4ad-222c-4e10-9014-b274f955f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9e4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'output/delta/accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef829dd8",
   "metadata": {},
   "source": [
    "### Create a Delta table\n",
    "\n",
    "* First drop the table accounts if it already exists\n",
    "  * use sql command to drop the table\n",
    "  * see [docs](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-drop-table.html#drop-table) for the drop command\n",
    "  * be aware that this doesn't remove the actual files, it just removes the information from metastore. You need to go and delete the actual files manually\n",
    "* Take the data from the `users_base_path` and save it as a delta table with the name `accounts`\n",
    "* as the location for the table use `accounts_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2f7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the table if it exists:\n",
    "\n",
    "spark.sql(\"drop table if exists accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddff3a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now create a new table accounts from the data\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .option('path', accounts_output_path)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362d4b9",
   "metadata": {},
   "source": [
    "### Verify that the table is created\n",
    "\n",
    "you can use the following SQL commands:\n",
    "* show tables\n",
    "* describe formatted table_name\n",
    "* describe extended table_name\n",
    "* describe detail table_name\n",
    "\n",
    "you can also use the API of the Delta table:\n",
    "* create the delta table object using the [DeltaTable.forName](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.forName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25066a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default| accounts|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if the table was successfully created:\n",
    "\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a924812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                              |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "|user_id                     |bigint                                                                 |NULL   |\n",
      "|display_name                |string                                                                 |NULL   |\n",
      "|about                       |string                                                                 |NULL   |\n",
      "|location                    |string                                                                 |NULL   |\n",
      "|downvotes                   |bigint                                                                 |NULL   |\n",
      "|upvotes                     |bigint                                                                 |NULL   |\n",
      "|reputation                  |bigint                                                                 |NULL   |\n",
      "|views                       |bigint                                                                 |NULL   |\n",
      "|                            |                                                                       |       |\n",
      "|# Detailed Table Information|                                                                       |       |\n",
      "|Name                        |spark_catalog.default.accounts                                         |       |\n",
      "|Type                        |EXTERNAL                                                               |       |\n",
      "|Location                    |file:/home/ubuntu/Apache-Spark-for-Data-Engineers/output/delta/accounts|       |\n",
      "|Provider                    |delta                                                                  |       |\n",
      "|Table Properties            |[delta.minReaderVersion=1,delta.minWriterVersion=2]                    |       |\n",
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('desc formatted accounts').show(truncate=False, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31ea8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/06 14:42:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:=================================================>       (43 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- format: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- lastModified: timestamp (nullable = true)\n",
      " |-- partitionColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clusteringColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- numFiles: long (nullable = true)\n",
      " |-- sizeInBytes: long (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- minReaderVersion: integer (nullable = true)\n",
      " |-- minWriterVersion: integer (nullable = true)\n",
      " |-- tableFeatures: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('describe detail accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa90c5a-fc88-43e2-991f-da2642309160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+-----------+----------+\n",
      "|                      location|numFiles|sizeInBytes|properties|\n",
      "+------------------------------+--------+-----------+----------+\n",
      "|file:/home/ubuntu/Apache-Sp...|       4|    3995164|        {}|\n",
      "+------------------------------+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe detail accounts').select('location', 'numFiles', 'sizeInBytes', 'properties').show(truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b73f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- format: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- lastModified: timestamp (nullable = true)\n",
      " |-- partitionColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clusteringColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- numFiles: long (nullable = true)\n",
      " |-- sizeInBytes: long (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- minReaderVersion: integer (nullable = true)\n",
      " |-- minWriterVersion: integer (nullable = true)\n",
      " |-- tableFeatures: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DeltaTable.forName(spark, 'accounts').detail().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c1e5b",
   "metadata": {},
   "source": [
    "### Version history of the delta table\n",
    "\n",
    "See the history of the delta table. You can use:\n",
    "* SQL command: describe history table_name\n",
    "* [history](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.history) function on the Delta table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bbecdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- jobRunId: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      " |-- engineInfo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe history accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68d5d29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|version|              timestamp|\n",
      "+-------+-----------------------+\n",
      "|      0|2024-10-06 14:41:24.372|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DeltaTable.forName(spark, 'accounts').history().select('version', 'timestamp').show(truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7273c",
   "metadata": {},
   "source": [
    "### Upsert / Merge\n",
    "\n",
    "* load the increment in to a Spark DataFrame \n",
    " * use the path users_increment_path\n",
    "* upsert the increment on the accounts table (use merge)\n",
    "* Useful links for merge:\n",
    " * docs for [merge](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.merge)\n",
    " * delta blog for [merge](https://delta.io/blog/2023-02-14-delta-lake-merge/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3eb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n",
    "\n",
    "increment = spark.read.parquet(users_increment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b50ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do the merge:\n",
    "\n",
    "(\n",
    "    DeltaTable.forName(spark, 'accounts')\n",
    "    .alias('accounts')\n",
    "    .merge(\n",
    "        increment.alias('increment'),\n",
    "        'accounts.user_id == increment.user_id'\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fafd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+----------------------------------------------------------------------------------------------------+\n",
      "|version|              timestamp|                                                                                    operationMetrics|\n",
      "+-------+-----------------------+----------------------------------------------------------------------------------------------------+\n",
      "|      3|2024-10-06 14:47:20.673|{numRemovedFiles -> 1, numRemovedBytes -> 4059419, numCopiedRows -> 124224, numDeletionVectorsAdd...|\n",
      "|      2|2024-10-06 14:46:46.875|{numRemovedFiles -> 4, numRemovedBytes -> 4120185, p25FileSize -> 4059419, numDeletionVectorsRemo...|\n",
      "|      1|2024-10-06 14:45:40.178|{numTargetRowsCopied -> 91560, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetByte...|\n",
      "|      0|2024-10-06 14:41:24.372|                                 {numFiles -> 4, numOutputRows -> 124225, numOutputBytes -> 3995164}|\n",
      "+-------+-----------------------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the history again to see that we have created a new version\n",
    "\n",
    "spark.sql('describe history accounts').select('version', 'timestamp', 'operationMetrics').show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dcc01",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "\n",
    "After you are done with the writes on the delta table, it might be useful to call optimize on it.\n",
    "\n",
    "* call [optimize](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.optimize)\n",
    "* use [z-order](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaOptimizeBuilder.executeZOrderBy) by the column `user_id`\n",
    "* check manually the files under the table to see that it was compacted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec03d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.forName(spark, 'accounts').optimize().executeZOrderBy('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534a308",
   "metadata": {},
   "source": [
    "### Delete from delta\n",
    "\n",
    "* [delete](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.delete) from the accounts table a row where user_id = 79\n",
    "* check that the row was really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60b75641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# delete:\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').delete(col('user_id') == 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "789a5203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that it was deleted:\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650d2f5",
   "metadata": {},
   "source": [
    "### Time travel\n",
    "\n",
    "* Now imagine, that you have made a mistake and you actually don't want to remove the user. Delta allows you to revert this operation. \n",
    "\n",
    "1) You can first take a look at a particular snapshot using the option `versionAsOf` on the DataFrameReader\n",
    "2) Then if you decide that you really want to revert your operation, you can use `restoreToVersion` on the Delta Table\n",
    "\n",
    "useful links:\n",
    "* docs for [restoreToVersion](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.restoreToVersion)\n",
    "* delta blog for [time travel](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)\n",
    "* delta blog for [rollback](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25c59af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the version of the table where the user wasn't deleted:\n",
    "\n",
    "spark.read.option(\"versionAsOf\", \"2\").table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bab8d25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/06 14:48:20 WARN DAGScheduler: Broadcasting large task binary with size 1078.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rollback to the version\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').restoreToVersion(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a0ab248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the table to see that the row is present there\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f334a",
   "metadata": {},
   "source": [
    "### Vacuum\n",
    "\n",
    "* remove old files that you don't need anymore\n",
    "* run [vacuum](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.vacuum) on the accounts delta table\n",
    "\n",
    "Note:\n",
    "* if you want to remove files that are not older then 7 days, you will have to disable the following setting: `spark.databricks.delta.retentionDurationCheck.enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ffa1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff3e9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.forName(spark, 'accounts').vacuum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ea56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
