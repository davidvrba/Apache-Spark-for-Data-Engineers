{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2382601e",
   "metadata": {},
   "source": [
    "# Delta Tables\n",
    "\n",
    "* In this notebook you will learn basic functionality of delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d004aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da357a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = configure_spark_with_delta_pip(\n",
    "    SparkSession.builder.appName('delta_tables_test')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    .enableHiveSupport()\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_base_path = os.path.join(project_path, 'data/users_base')\n",
    "users_increment_path = os.path.join(project_path, 'data/users_increment')\n",
    "accounts_output_path = os.path.join(project_path, 'data/delta/accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef829dd8",
   "metadata": {},
   "source": [
    "### Create a Delta table\n",
    "\n",
    "* First drop the table accounts if it already exists\n",
    "  * use sql command to drop the table\n",
    "  * see [docs](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-drop-table.html#drop-table) for the drop command\n",
    "  * be aware that this doesn't remove the actual files, it just removes the information from metastore. You need to go and delete the actual files manually\n",
    "* Take the data from the `users_base_path` and save it as a delta table with the name `accounts`\n",
    "* as the location for the table use `accounts_output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the table if it exists:\n",
    "\n",
    "spark.sql(\"drop table if exists accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a new table accounts from the data\n",
    "(\n",
    "    spark.read.parquet(users_base_path)\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .option('path', accounts_output_path)\n",
    "    .saveAsTable('accounts')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362d4b9",
   "metadata": {},
   "source": [
    "### Verify that the table is created\n",
    "\n",
    "you can use the following SQL commands:\n",
    "* show tables\n",
    "* describe formatted table_name\n",
    "* describe extended table_name\n",
    "* describe detail table_name\n",
    "\n",
    "you can also use the API of the Delta table:\n",
    "* create the delta table object using the [DeltaTable.forName](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.forName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25066a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the table was successfully created:\n",
    "\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('desc formatted accounts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ea8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe detail accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b73f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').detail().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c1e5b",
   "metadata": {},
   "source": [
    "### Version history of the delta table\n",
    "\n",
    "See the history of the delta table. You can use:\n",
    "* SQL command: describe history table_name\n",
    "* [history](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.history) function on the Delta table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbecdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe history accounts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').history().select('version', 'timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7273c",
   "metadata": {},
   "source": [
    "### Upsert / Merge\n",
    "\n",
    "* load the increment in to a Spark DataFrame \n",
    " * use the path users_increment_path\n",
    "* upsert the increment on the accounts table (use merge)\n",
    "* Useful links for merge:\n",
    " * docs for [merge](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.merge)\n",
    " * delta blog for [merge](https://delta.io/blog/2023-02-14-delta-lake-merge/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the increment:\n",
    "\n",
    "increment = spark.read.parquet(users_increment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the merge:\n",
    "\n",
    "(\n",
    "    DeltaTable.forName(spark, 'accounts')\n",
    "    .alias('accounts')\n",
    "    .merge(\n",
    "        increment.alias('increment'),\n",
    "        'accounts.user_id == increment.user_id'\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the history again to see that we have created a new version\n",
    "\n",
    "spark.sql('describe history accounts').select('version', 'timestamp', 'operationMetrics').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dcc01",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "\n",
    "After you are done with the writes on the delta table, it might be useful to call optimize on it.\n",
    "\n",
    "* call [optimize](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.optimize)\n",
    "* use [z-order](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaOptimizeBuilder.executeZOrderBy) by the column `user_id`\n",
    "* check manually the files under the table to see that it was compacted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec03d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').optimize().executeZOrderBy('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534a308",
   "metadata": {},
   "source": [
    "### Delete from delta\n",
    "\n",
    "* [delete](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.delete) from the accounts table a row where user_id = 79\n",
    "* check that the row was really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b75641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete:\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').delete(col('user_id') == 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it was deleted:\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650d2f5",
   "metadata": {},
   "source": [
    "### Time travel\n",
    "\n",
    "* Now imagine, that you have made a mistake and you actually don't want to remove the user. Delta allows you to revert this operation. \n",
    "\n",
    "1) You can first take a look at a particular snapshot using the option `versionAsOf` on the DataFrameReader\n",
    "2) Then if you decide that you really want to revert your operation, you can use `restoreToVersion` on the Delta Table\n",
    "\n",
    "useful links:\n",
    "* docs for [restoreToVersion](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.restoreToVersion)\n",
    "* delta blog for [time travel](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)\n",
    "* delta blog for [rollback](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c59af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the version of the table where the user wasn't deleted:\n",
    "\n",
    "spark.read.option(\"versionAsOf\", \"2\").table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollback to the version\n",
    "\n",
    "DeltaTable.forName(spark, 'accounts').restoreToVersion(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ab248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the table to see that the row is present there\n",
    "\n",
    "spark.table('accounts').filter(col('user_id') == 79).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f334a",
   "metadata": {},
   "source": [
    "### Vacuum\n",
    "\n",
    "* remove old files that you don't need anymore\n",
    "* run [vacuum](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaTable.vacuum) on the accounts delta table\n",
    "\n",
    "Note:\n",
    "* if you want to remove files that are not older then 7 days, you will have to disable the following setting: `spark.databricks.delta.retentionDurationCheck.enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, 'accounts').vacuum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ea56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark Delta Python 3.10.1",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
